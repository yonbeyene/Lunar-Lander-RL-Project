{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e522867",
   "metadata": {},
   "source": [
    "## SARSA with 10 value discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb8a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96b900ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_states(observation):\n",
    "\n",
    "    ds_vector = np.zeros(6)\n",
    "    ds=-1\n",
    "    dis_number = 10\n",
    "    \n",
    "    \n",
    "    #Discretize x\n",
    "    if observation[0] <= -1:\n",
    "        ds_vector[0] = -1\n",
    "    elif observation[0] >= 1:\n",
    "        ds_vector[0] = 1\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -1 <= observation[0] <= (-1 + i*2/dis_number):\n",
    "                ds_vector[0] = -1 + i*2/dis_number\n",
    "                break\n",
    "\n",
    "\n",
    "    #Discretize y\n",
    "    if observation[1] <= -1:\n",
    "        ds_vector[1] = -1\n",
    "    elif observation[1] >= 1:\n",
    "        ds_vector[1] = 1\n",
    "\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -1 <= observation[1] <= (-1 + i*2/dis_number):\n",
    "                ds_vector[1] = -1 + i*2/dis_number \n",
    "                break\n",
    "                \n",
    "                \n",
    "    #Discretize Vx\n",
    "    if observation[2] <= -1.5:\n",
    "        ds_vector[2] = -1.5\n",
    "    elif observation[2] >= 1.5:\n",
    "        ds_vector[2] = 1.5\n",
    "\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -1.5 <= observation[2] <= (-1.5 + i*3/dis_number):\n",
    "                ds_vector[2] = -1.5 + i*3/dis_number \n",
    "                break    \n",
    "    \n",
    "    \n",
    "    #Discretize Vy\n",
    "    if observation[3] <= -1.5:\n",
    "        ds_vector[3] = -1.5\n",
    "    elif observation[3] >= 1.5:\n",
    "        ds_vector[3] = 1.5\n",
    "\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -1.5 <= observation[3] <= (-1.5 + i*3/dis_number):\n",
    "                ds_vector[3] = -1.5 + i*3/dis_number \n",
    "                break        \n",
    "    \n",
    "    #Discretize theta\n",
    "    if observation[4] <= -2:\n",
    "        ds_vector[4] = -2\n",
    "    elif observation[4] >= 2:\n",
    "        ds_vector[4] = 2\n",
    "\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -2 <= observation[4] <= (-2 + i*4/dis_number):\n",
    "                ds_vector[4] = -2 + i*4/dis_number \n",
    "                break    \n",
    "    \n",
    "    \n",
    "    #Discretize V_theta\n",
    "    if observation[5] <= -6:\n",
    "        ds_vector[5] = -6\n",
    "    elif observation[5] >= 6:\n",
    "        ds_vector[5] = 6\n",
    "\n",
    "    else:\n",
    "        for i in range(dis_number):\n",
    "            if -6 <= observation[5] <= (-6 + i*12/dis_number):\n",
    "                ds_vector[5] = -6 + i*12/dis_number \n",
    "                break    \n",
    "                \n",
    "    ds_vector[6] = observation[6]\n",
    "    ds_vector[7] = observation[7]\n",     
    "    return ds_vector\n",
    "\n",
    "\n",
    "\n",
    "def sa_key(s, a):\n",
    "    return str(s) + \" \" + str(a)\n",
    "\n",
    "\n",
    "def policy_explorer(s, Q, epsilon=20):\n",
    "    rand = np.random.randint(0, 100)\n",
    "\n",
    "    if rand >= epsilon:\n",
    "        Qv = np.array([ Q[sa_key(s, action)] for action in [0, 1, 2, 3]])\n",
    "        return np.argmax(Qv)\n",
    "    else:\n",
    "        return np.random.randint(0, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6c94d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#It:  0  avg reward:  -270.76288673481804  out of  1  trials\n",
      "#It:  100  avg reward:  -151.3316559044723  out of  100  trials\n",
      "#It:  200  avg reward:  -201.0847388572972  out of  100  trials\n",
      "#It:  300  avg reward:  -167.51017130558554  out of  100  trials\n",
      "#It:  400  avg reward:  -183.28240188606392  out of  100  trials\n",
      "#It:  500  avg reward:  -180.57487581029562  out of  100  trials\n",
      "#It:  600  avg reward:  -181.07601001970994  out of  100  trials\n",
      "#It:  700  avg reward:  -188.65748215688754  out of  100  trials\n",
      "#It:  800  avg reward:  -176.55047523177464  out of  100  trials\n",
      "#It:  900  avg reward:  -173.50428412735414  out of  100  trials\n",
      "#It:  1000  avg reward:  -182.63355617796734  out of  100  trials\n",
      "#It:  1100  avg reward:  -181.74209219686963  out of  100  trials\n",
      "#It:  1200  avg reward:  -202.9611884873548  out of  100  trials\n",
      "#It:  1300  avg reward:  -195.339948063424  out of  100  trials\n",
      "#It:  1400  avg reward:  -197.19200395417704  out of  100  trials\n",
      "#It:  1500  avg reward:  -174.74035504193225  out of  100  trials\n",
      "#It:  1600  avg reward:  -159.200917050381  out of  100  trials\n",
      "#It:  1700  avg reward:  -165.29237883597037  out of  100  trials\n",
      "#It:  1800  avg reward:  -163.28603657368942  out of  100  trials\n",
      "#It:  1900  avg reward:  -184.14477796375775  out of  100  trials\n",
      "#It:  2000  avg reward:  -191.60800929829423  out of  100  trials\n",
      "#It:  2100  avg reward:  -169.48929963532362  out of  100  trials\n",
      "#It:  2200  avg reward:  -183.77691761985045  out of  100  trials\n",
      "#It:  2300  avg reward:  -202.84839750951048  out of  100  trials\n",
      "#It:  2400  avg reward:  -185.78733505270512  out of  100  trials\n",
      "#It:  2500  avg reward:  -176.94076857749505  out of  100  trials\n",
      "#It:  2600  avg reward:  -185.06052978119135  out of  100  trials\n",
      "#It:  2700  avg reward:  -208.89386295203093  out of  100  trials\n",
      "#It:  2800  avg reward:  -196.7499531141484  out of  100  trials\n",
      "#It:  2900  avg reward:  -191.18938364367946  out of  100  trials\n",
      "#It:  3000  avg reward:  -175.48462466887065  out of  100  trials\n",
      "#It:  3100  avg reward:  -178.64151640624007  out of  100  trials\n",
      "#It:  3200  avg reward:  -185.79816776986678  out of  100  trials\n",
      "#It:  3300  avg reward:  -188.8775231607637  out of  100  trials\n",
      "#It:  3400  avg reward:  -204.36732113957328  out of  100  trials\n",
      "#It:  3500  avg reward:  -166.86523930232616  out of  100  trials\n",
      "#It:  3600  avg reward:  -162.622411178316  out of  100  trials\n",
      "#It:  3700  avg reward:  -164.81456447592217  out of  100  trials\n",
      "#It:  3800  avg reward:  -190.8409628283133  out of  100  trials\n",
      "#It:  3900  avg reward:  -186.70048666804317  out of  100  trials\n",
      "#It:  4000  avg reward:  -178.29286550622749  out of  100  trials\n",
      "#It:  4100  avg reward:  -177.54504332315412  out of  100  trials\n",
      "#It:  4200  avg reward:  -176.80753384678255  out of  100  trials\n",
      "#It:  4300  avg reward:  -175.85486258996727  out of  100  trials\n",
      "#It:  4400  avg reward:  -195.78555137407642  out of  100  trials\n",
      "#It:  4500  avg reward:  -179.62624009390765  out of  100  trials\n",
      "#It:  4600  avg reward:  -190.79648171994887  out of  100  trials\n",
      "#It:  4700  avg reward:  -198.71277584762404  out of  100  trials\n",
      "#It:  4800  avg reward:  -198.9049670165364  out of  100  trials\n",
      "#It:  4900  avg reward:  -171.762236611335  out of  100  trials\n",
      "#It:  5000  avg reward:  -193.17155960660776  out of  100  trials\n",
      "#It:  5100  avg reward:  -179.78877218764748  out of  100  trials\n",
      "#It:  5200  avg reward:  -185.7867191726151  out of  100  trials\n",
      "#It:  5300  avg reward:  -189.73479625452012  out of  100  trials\n",
      "#It:  5400  avg reward:  -181.23084675040315  out of  100  trials\n",
      "#It:  5500  avg reward:  -184.67431453157081  out of  100  trials\n",
      "#It:  5600  avg reward:  -192.53797155508616  out of  100  trials\n",
      "#It:  5700  avg reward:  -168.0058666449271  out of  100  trials\n",
      "#It:  5800  avg reward:  -206.14407070373494  out of  100  trials\n",
      "#It:  5900  avg reward:  -183.58532435945523  out of  100  trials\n",
      "#It:  6000  avg reward:  -167.95206571772215  out of  100  trials\n",
      "#It:  6100  avg reward:  -199.29630592848764  out of  100  trials\n",
      "#It:  6200  avg reward:  -207.95395787337839  out of  100  trials\n",
      "#It:  6300  avg reward:  -189.06692281675106  out of  100  trials\n",
      "#It:  6400  avg reward:  -185.8571434524472  out of  100  trials\n",
      "#It:  6500  avg reward:  -195.49426149781658  out of  100  trials\n",
      "#It:  6600  avg reward:  -178.3900170229982  out of  100  trials\n",
      "#It:  6700  avg reward:  -179.43267323217927  out of  100  trials\n",
      "#It:  6800  avg reward:  -182.7223911915645  out of  100  trials\n",
      "#It:  6900  avg reward:  -181.88613866120235  out of  100  trials\n",
      "#It:  7000  avg reward:  -169.77874662913052  out of  100  trials\n",
      "#It:  7100  avg reward:  -174.55462042920294  out of  100  trials\n",
      "#It:  7200  avg reward:  -186.21821738984818  out of  100  trials\n",
      "#It:  7300  avg reward:  -176.48671454473396  out of  100  trials\n",
      "#It:  7400  avg reward:  -191.74878430532837  out of  100  trials\n",
      "#It:  7500  avg reward:  -187.8778488814647  out of  100  trials\n",
      "#It:  7600  avg reward:  -187.60401818199588  out of  100  trials\n",
      "#It:  7700  avg reward:  -169.69029126932787  out of  100  trials\n",
      "#It:  7800  avg reward:  -172.35183633746183  out of  100  trials\n",
      "#It:  7900  avg reward:  -168.4964949628977  out of  100  trials\n",
      "#It:  8000  avg reward:  -174.3261352655604  out of  100  trials\n",
      "#It:  8100  avg reward:  -182.4464098669805  out of  100  trials\n",
      "#It:  8200  avg reward:  -183.51848888421807  out of  100  trials\n",
      "#It:  8300  avg reward:  -186.1249392615317  out of  100  trials\n",
      "#It:  8400  avg reward:  -186.92925011799187  out of  100  trials\n",
      "#It:  8500  avg reward:  -198.83425285889393  out of  100  trials\n",
      "#It:  8600  avg reward:  -177.32937954380967  out of  100  trials\n",
      "#It:  8700  avg reward:  -169.7776818201081  out of  100  trials\n",
      "#It:  8800  avg reward:  -178.54192363587902  out of  100  trials\n",
      "#It:  8900  avg reward:  -182.56691410438674  out of  100  trials\n",
      "#It:  9000  avg reward:  -181.96212356287754  out of  100  trials\n",
      "#It:  9100  avg reward:  -172.71405080597847  out of  100  trials\n",
      "#It:  9200  avg reward:  -183.69481049073386  out of  100  trials\n",
      "#It:  9300  avg reward:  -192.3746332844473  out of  100  trials\n",
      "#It:  9400  avg reward:  -173.34951527426065  out of  100  trials\n",
      "#It:  9500  avg reward:  -165.4761619711711  out of  100  trials\n",
      "#It:  9600  avg reward:  -197.3806876084309  out of  100  trials\n",
      "#It:  9700  avg reward:  -172.53815750235609  out of  100  trials\n",
      "#It:  9800  avg reward:  -189.27691694068545  out of  100  trials\n",
      "#It:  9900  avg reward:  -174.02663730059714  out of  100  trials\n"
     ]
    }
   ],
   "source": [
    "def sarsa_lander(env, seed=None, render=False, num_iter=50, seg=50):\n",
    "    env.seed(42)\n",
    "\n",
    "    Q = collections.defaultdict(float)\n",
    "\n",
    "    gamma = 0.95\n",
    "\n",
    "    r_seq = []\n",
    "    it_reward = []\n",
    "\n",
    "    for it in range(num_iter):\n",
    "        # initialize variables\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        lr = 0.6\n",
    "\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "\n",
    "        ds = discrete_states(s)\n",
    "        a = policy_explorer(ds, Q, it)\n",
    "        # start Sarsa\n",
    "        while True:\n",
    "            sa = sa_key(ds, a)\n",
    "            if render:\n",
    "                env.render()\n",
    "            sp, r, done, info = env.step(a)\n",
    "            # update corresponding Q\n",
    "            dsp = discrete_states(sp)\n",
    "            ap = policy_explorer(dsp, Q, it)\n",
    "            next_sa = sa_key(dsp, ap)\n",
    "            if not done:\n",
    "                Q[sa] += lr*(r + gamma * Q[next_sa] - Q[sa])\n",
    "            else:\n",
    "                Q[sa] += lr*(r - Q[sa])\n",
    "            ds = dsp\n",
    "            a = ap\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            if done or steps > 1000:\n",
    "                it_reward.append(total_reward)\n",
    "                break\n",
    "        if it % seg == 0:\n",
    "            avg_rwd = np.mean(np.array(it_reward))\n",
    "            print(\"#It: \", it, \" avg reward: \", avg_rwd, \" out of \", len(it_reward), \" trials\")\n",
    "            it_reward = []\n",
    "            r_seq.append(avg_rwd)\n",
    "\n",
    "    return Q, r_seq\n",
    "\n",
    "num_iter = 10000\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "Q, r_seq = sarsa_lander(env, render=False, num_iter=num_iter, seg=100)\n",
    "\n",
    "\n",
    "y = np.array(r_seq)\n",
    "x = np.linspace(0, num_iter, y.shape[0])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(x, y)\n",
    "plt.title('10 value discretized SARSA')\n",
    "plt.savefig(\"10 value discretized SARSA\")\n",
    "plt.close()\n",
    "\n",
    "np.savetxt(\"10 v sarsa.txt\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4307c4",
   "metadata": {},
   "source": [
    "## Q-Learning with 10-State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0602ceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#It:  0  avg reward:  -126.28280264053126  out of  1  trials\n",
      "#It:  100  avg reward:  -159.06212688537084  out of  100  trials\n",
      "#It:  200  avg reward:  -145.1948299140876  out of  100  trials\n",
      "#It:  300  avg reward:  -138.0241256207595  out of  100  trials\n",
      "#It:  400  avg reward:  -151.7131809690659  out of  100  trials\n",
      "#It:  500  avg reward:  -117.07075064272806  out of  100  trials\n",
      "#It:  600  avg reward:  -112.74028272597319  out of  100  trials\n",
      "#It:  700  avg reward:  -127.2454259802638  out of  100  trials\n",
      "#It:  800  avg reward:  -104.72805764074965  out of  100  trials\n",
      "#It:  900  avg reward:  -123.98609797971591  out of  100  trials\n",
      "#It:  1000  avg reward:  -115.77207069122045  out of  100  trials\n",
      "#It:  1100  avg reward:  -128.10005793801017  out of  100  trials\n",
      "#It:  1200  avg reward:  -125.10144070557935  out of  100  trials\n",
      "#It:  1300  avg reward:  -143.5302315308204  out of  100  trials\n",
      "#It:  1400  avg reward:  -137.40102339788177  out of  100  trials\n",
      "#It:  1500  avg reward:  -112.85638691493823  out of  100  trials\n",
      "#It:  1600  avg reward:  -116.23061989909647  out of  100  trials\n",
      "#It:  1700  avg reward:  -109.11349415851944  out of  100  trials\n",
      "#It:  1800  avg reward:  -122.05215782236726  out of  100  trials\n",
      "#It:  1900  avg reward:  -109.54973425997406  out of  100  trials\n",
      "#It:  2000  avg reward:  -112.82831275971974  out of  100  trials\n",
      "#It:  2100  avg reward:  -114.99352736882673  out of  100  trials\n",
      "#It:  2200  avg reward:  -129.56153870961847  out of  100  trials\n",
      "#It:  2300  avg reward:  -125.09948312029987  out of  100  trials\n",
      "#It:  2400  avg reward:  -106.01680933695074  out of  100  trials\n",
      "#It:  2500  avg reward:  -101.56068129949796  out of  100  trials\n",
      "#It:  2600  avg reward:  -91.04853508689068  out of  100  trials\n",
      "#It:  2700  avg reward:  -87.81004728184018  out of  100  trials\n",
      "#It:  2800  avg reward:  -98.12019185539079  out of  100  trials\n",
      "#It:  2900  avg reward:  -109.84100311277514  out of  100  trials\n",
      "#It:  3000  avg reward:  -96.52543105544119  out of  100  trials\n",
      "#It:  3100  avg reward:  -109.11253111754674  out of  100  trials\n",
      "#It:  3200  avg reward:  -104.85356579969448  out of  100  trials\n",
      "#It:  3300  avg reward:  -113.33795983274021  out of  100  trials\n",
      "#It:  3400  avg reward:  -110.92978204581193  out of  100  trials\n",
      "#It:  3500  avg reward:  -104.57587734552128  out of  100  trials\n",
      "#It:  3600  avg reward:  -100.1637489525973  out of  100  trials\n",
      "#It:  3700  avg reward:  -113.64813906592117  out of  100  trials\n",
      "#It:  3800  avg reward:  -107.94399573046056  out of  100  trials\n",
      "#It:  3900  avg reward:  -99.17159255767177  out of  100  trials\n",
      "#It:  4000  avg reward:  -100.96471651770163  out of  100  trials\n",
      "#It:  4100  avg reward:  -103.35046870787899  out of  100  trials\n",
      "#It:  4200  avg reward:  -98.17326478331044  out of  100  trials\n",
      "#It:  4300  avg reward:  -92.57049419889516  out of  100  trials\n",
      "#It:  4400  avg reward:  -116.0468502680379  out of  100  trials\n",
      "#It:  4500  avg reward:  -93.18642815568037  out of  100  trials\n",
      "#It:  4600  avg reward:  -104.44204194527066  out of  100  trials\n",
      "#It:  4700  avg reward:  -116.44919954398594  out of  100  trials\n",
      "#It:  4800  avg reward:  -119.95528712162506  out of  100  trials\n",
      "#It:  4900  avg reward:  -92.52367619289338  out of  100  trials\n",
      "#It:  5000  avg reward:  -118.0432480802691  out of  100  trials\n",
      "#It:  5100  avg reward:  -121.1633375200783  out of  100  trials\n",
      "#It:  5200  avg reward:  -123.50175805448694  out of  100  trials\n",
      "#It:  5300  avg reward:  -105.87105309203332  out of  100  trials\n",
      "#It:  5400  avg reward:  -106.91540251024543  out of  100  trials\n",
      "#It:  5500  avg reward:  -100.86523569040583  out of  100  trials\n",
      "#It:  5600  avg reward:  -96.09535962797189  out of  100  trials\n",
      "#It:  5700  avg reward:  -119.54560921366924  out of  100  trials\n",
      "#It:  5800  avg reward:  -118.07364372281029  out of  100  trials\n",
      "#It:  5900  avg reward:  -120.26758828132517  out of  100  trials\n",
      "#It:  6000  avg reward:  -114.19156068778064  out of  100  trials\n",
      "#It:  6100  avg reward:  -101.8129565232896  out of  100  trials\n",
      "#It:  6200  avg reward:  -110.7064387041221  out of  100  trials\n",
      "#It:  6300  avg reward:  -107.19937225501246  out of  100  trials\n",
      "#It:  6400  avg reward:  -107.81484625529303  out of  100  trials\n",
      "#It:  6500  avg reward:  -110.32213901818945  out of  100  trials\n",
      "#It:  6600  avg reward:  -102.59545578320433  out of  100  trials\n",
      "#It:  6700  avg reward:  -129.07938969569162  out of  100  trials\n",
      "#It:  6800  avg reward:  -110.13647625459251  out of  100  trials\n",
      "#It:  6900  avg reward:  -93.98574948482333  out of  100  trials\n",
      "#It:  7000  avg reward:  -122.41932050450959  out of  100  trials\n",
      "#It:  7100  avg reward:  -127.74811814056126  out of  100  trials\n",
      "#It:  7200  avg reward:  -123.99793206975133  out of  100  trials\n",
      "#It:  7300  avg reward:  -106.73562538171167  out of  100  trials\n",
      "#It:  7400  avg reward:  -124.30787624731424  out of  100  trials\n",
      "#It:  7500  avg reward:  -119.45793915819426  out of  100  trials\n",
      "#It:  7600  avg reward:  -130.50962081542855  out of  100  trials\n",
      "#It:  7700  avg reward:  -111.09279092416635  out of  100  trials\n",
      "#It:  7800  avg reward:  -114.9338100937285  out of  100  trials\n",
      "#It:  7900  avg reward:  -109.63079836057237  out of  100  trials\n",
      "#It:  8000  avg reward:  -105.97348353873694  out of  100  trials\n",
      "#It:  8100  avg reward:  -112.64759636497924  out of  100  trials\n",
      "#It:  8200  avg reward:  -119.4855928514418  out of  100  trials\n",
      "#It:  8300  avg reward:  -107.40065720945958  out of  100  trials\n",
      "#It:  8400  avg reward:  -131.72583359348533  out of  100  trials\n",
      "#It:  8500  avg reward:  -101.21872682542622  out of  100  trials\n",
      "#It:  8600  avg reward:  -113.6084795747338  out of  100  trials\n",
      "#It:  8700  avg reward:  -98.6648235175246  out of  100  trials\n",
      "#It:  8800  avg reward:  -112.60683785556692  out of  100  trials\n",
      "#It:  8900  avg reward:  -122.32416997773082  out of  100  trials\n",
      "#It:  9000  avg reward:  -105.05680645814327  out of  100  trials\n",
      "#It:  9100  avg reward:  -98.78245082561547  out of  100  trials\n",
      "#It:  9200  avg reward:  -114.05306008084203  out of  100  trials\n",
      "#It:  9300  avg reward:  -104.89079265423223  out of  100  trials\n",
      "#It:  9400  avg reward:  -111.69213743857959  out of  100  trials\n",
      "#It:  9500  avg reward:  -125.55983597362723  out of  100  trials\n",
      "#It:  9600  avg reward:  -108.18413480300467  out of  100  trials\n",
      "#It:  9700  avg reward:  -114.04830728818855  out of  100  trials\n",
      "#It:  9800  avg reward:  -115.0213112649106  out of  100  trials\n",
      "#It:  9900  avg reward:  -98.75713032802832  out of  100  trials\n"
     ]
    }
   ],
   "source": [
    "def qlearning_lander(env, learningrate=0.01, render=True, num_iter=100, seg=100):\n",
    "    Q = collections.defaultdict(float)\n",
    "    gamma = 0.95\n",
    "    r_seq = []\n",
    "    it_reward = []\n",
    "\n",
    "    for it in range(num_iter):\n",
    "        # initialize variables\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "\n",
    "        ds = discrete_states(s)\n",
    "        a = policy_explorer(ds, Q) #initial action is e-greedy\n",
    "        # start Sarsa\n",
    "        while True:\n",
    "            # discrete state ds initialized either before loop or at end of previous step\n",
    "            a = policy_explorer(ds, Q) #initial action is e-greedy\n",
    "            sa = sa_key(ds, a)\n",
    "            if render:\n",
    "                env.render()\n",
    "            sp, r, done, info = env.step(a)\n",
    "            # update corresponding Q\n",
    "            dsp = discrete_states(sp)\n",
    "            a_max = policy_explorer(dsp, Q, epsilon=0) #now select action as pure-greedy\n",
    "            next_sa = sa_key(dsp, a_max)\n",
    "            if not done:\n",
    "                Q[sa] += learningrate*(r + gamma * Q[next_sa] - Q[sa])\n",
    "            else:\n",
    "                Q[sa] += learningrate*(r - Q[sa])\n",
    "            ds = dsp\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            if done or steps > 1000:\n",
    "                it_reward.append(total_reward)\n",
    "                break\n",
    "        if it % seg == 0:\n",
    "            avg_rwd = np.mean(np.array(it_reward))\n",
    "            print(\"#It: \", it, \" avg reward: \", avg_rwd, \" out of \", len(it_reward), \" trials\")\n",
    "            it_reward = []\n",
    "            r_seq.append(avg_rwd)\n",
    "\n",
    "    return Q, r_seq\n",
    "\n",
    "num_iter = 10000\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "alphas = [0.6]\n",
    "for a in alphas:\n",
    "    Q, r_seq = qlearning_lander(env, learningrate=a, render=False, num_iter=num_iter, seg=100)\n",
    "    \n",
    "y = np.array(r_seq)\n",
    "x = np.linspace(0, num_iter, y.shape[0])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.title(f'10 value discretized Q-Learning')\n",
    "plt.savefig(f\"10 value discretized Q-Learning\")\n",
    "plt.close()\n",
    "\n",
    "np.savetxt(\"10 v Q-Learning.txt\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5106a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
