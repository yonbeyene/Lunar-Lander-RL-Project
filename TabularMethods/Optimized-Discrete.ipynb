{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144a70e",
   "metadata": {},
   "source": [
    "## SARSA with Optimized Discretization of States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c799027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reference: Soham Gadgil, Yunfeng Xin, and Chengzhe Xu. “Solving the lunar lander problem under uncertainty using\n",
    "reinforcement learning”. In: 2020 SoutheastCon. Vol. 2. IEEE. 2020, pp. 1–8.\n",
    "'''\n",
    "\n",
    "import collections, gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import import_ipynb\n",
    "# import lunar_lander as lander\n",
    "\n",
    "num_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32790482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discrete_states(s):\n",
    "    state = (min(5, max(-5, int((s[0]) / 0.05))), \\\n",
    "            min(5, max(-1, int((s[1]) / 0.1))), \\\n",
    "            min(3, max(-3, int((s[2]) / 0.1))), \\\n",
    "            min(3, max(-3, int((s[3]) / 0.1))), \\\n",
    "            min(3, max(-3, int((s[4]) / 0.1))), \\\n",
    "            min(3, max(-3, int((s[5]) / 0.1))), \\\n",
    "            int(s[6]), \\\n",
    "            int(s[7]))\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def sa_key(s, a):\n",
    "    return str(s) + \" \" + str(a)\n",
    "\n",
    "\n",
    "def policy_explorer(s, Q, iter, epsilon=50):\n",
    "    rand = np.random.randint(0, 100)\n",
    "    if iter > 200:\n",
    "        epsilon = 10\n",
    "    if iter > 2000:\n",
    "        epsilon = 5\n",
    "    if iter > 5000:\n",
    "        epsilon = 1\n",
    "    if iter > 7500:\n",
    "        epsilon = 0\n",
    "\n",
    "    if rand >= epsilon:\n",
    "        Qv = np.array([ Q[sa_key(s, action)] for action in [0, 1, 2, 3]])\n",
    "        return np.argmax(Qv)\n",
    "    else:\n",
    "        return np.random.randint(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a055aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#It:  0  avg reward:  -123.50767522226923  out of  1  trials\n",
      "#It:  100  avg reward:  -154.67881791532855  out of  100  trials\n",
      "#It:  200  avg reward:  -132.5644927010207  out of  100  trials\n",
      "#It:  300  avg reward:  -119.39376858129397  out of  100  trials\n",
      "#It:  400  avg reward:  -113.75123054833745  out of  100  trials\n",
      "#It:  500  avg reward:  -98.37469327941379  out of  100  trials\n",
      "#It:  600  avg reward:  -108.50034971341366  out of  100  trials\n",
      "#It:  700  avg reward:  -113.96113863639437  out of  100  trials\n",
      "#It:  800  avg reward:  -114.75605203256808  out of  100  trials\n",
      "#It:  900  avg reward:  -112.04838291388563  out of  100  trials\n",
      "#It:  1000  avg reward:  -94.33880328512008  out of  100  trials\n",
      "#It:  1100  avg reward:  -113.30820170141833  out of  100  trials\n",
      "#It:  1200  avg reward:  -108.77540549866767  out of  100  trials\n",
      "#It:  1300  avg reward:  -94.7367894011091  out of  100  trials\n",
      "#It:  1400  avg reward:  -114.85590426688753  out of  100  trials\n",
      "#It:  1500  avg reward:  -85.49280644667104  out of  100  trials\n",
      "#It:  1600  avg reward:  -94.60597899600279  out of  100  trials\n",
      "#It:  1700  avg reward:  -104.55820327497605  out of  100  trials\n",
      "#It:  1800  avg reward:  -113.90340263573971  out of  100  trials\n",
      "#It:  1900  avg reward:  -90.58577874562134  out of  100  trials\n",
      "#It:  2000  avg reward:  -107.54675355402763  out of  100  trials\n",
      "#It:  2100  avg reward:  -104.10832846460602  out of  100  trials\n",
      "#It:  2200  avg reward:  -105.79782550050471  out of  100  trials\n",
      "#It:  2300  avg reward:  -113.77860201306501  out of  100  trials\n",
      "#It:  2400  avg reward:  -125.27145237374519  out of  100  trials\n",
      "#It:  2500  avg reward:  -96.18857255539942  out of  100  trials\n",
      "#It:  2600  avg reward:  -96.70637422520866  out of  100  trials\n",
      "#It:  2700  avg reward:  -90.41870302484571  out of  100  trials\n",
      "#It:  2800  avg reward:  -96.9367624519273  out of  100  trials\n",
      "#It:  2900  avg reward:  -97.15297573522507  out of  100  trials\n",
      "#It:  3000  avg reward:  -98.97293747957397  out of  100  trials\n",
      "#It:  3100  avg reward:  -101.33400314024692  out of  100  trials\n",
      "#It:  3200  avg reward:  -102.9190133503276  out of  100  trials\n",
      "#It:  3300  avg reward:  -83.64462858191143  out of  100  trials\n",
      "#It:  3400  avg reward:  -75.20918130723207  out of  100  trials\n",
      "#It:  3500  avg reward:  -72.31130952877997  out of  100  trials\n",
      "#It:  3600  avg reward:  -86.81956383245335  out of  100  trials\n",
      "#It:  3700  avg reward:  -85.69941800076639  out of  100  trials\n",
      "#It:  3800  avg reward:  -93.14317992757928  out of  100  trials\n",
      "#It:  3900  avg reward:  -86.22730103641686  out of  100  trials\n",
      "#It:  4000  avg reward:  -87.18177569769112  out of  100  trials\n",
      "#It:  4100  avg reward:  -89.54878335347703  out of  100  trials\n",
      "#It:  4200  avg reward:  -77.71916409209474  out of  100  trials\n",
      "#It:  4300  avg reward:  -71.70705460120348  out of  100  trials\n",
      "#It:  4400  avg reward:  -84.36178030044319  out of  100  trials\n",
      "#It:  4500  avg reward:  -81.56626034985183  out of  100  trials\n",
      "#It:  4600  avg reward:  -71.20356260613863  out of  100  trials\n",
      "#It:  4700  avg reward:  -84.36256642946482  out of  100  trials\n",
      "#It:  4800  avg reward:  -93.15314148283532  out of  100  trials\n",
      "#It:  4900  avg reward:  -81.33791918209035  out of  100  trials\n",
      "#It:  5000  avg reward:  -66.54720235075725  out of  100  trials\n",
      "#It:  5100  avg reward:  -66.74571232283346  out of  100  trials\n",
      "#It:  5200  avg reward:  -71.25153328770037  out of  100  trials\n",
      "#It:  5300  avg reward:  -59.41297715160324  out of  100  trials\n",
      "#It:  5400  avg reward:  -86.97844335052176  out of  100  trials\n",
      "#It:  5500  avg reward:  -57.37837160203957  out of  100  trials\n",
      "#It:  5600  avg reward:  -74.04799403502423  out of  100  trials\n",
      "#It:  5700  avg reward:  -60.61704856025711  out of  100  trials\n",
      "#It:  5800  avg reward:  -63.33978841033929  out of  100  trials\n",
      "#It:  5900  avg reward:  -70.82677683617952  out of  100  trials\n",
      "#It:  6000  avg reward:  -54.33156071876649  out of  100  trials\n",
      "#It:  6100  avg reward:  -59.09602033453254  out of  100  trials\n",
      "#It:  6200  avg reward:  -39.659264633513295  out of  100  trials\n",
      "#It:  6300  avg reward:  -61.879206822545896  out of  100  trials\n",
      "#It:  6400  avg reward:  -39.047640292681926  out of  100  trials\n",
      "#It:  6500  avg reward:  -49.956839660140204  out of  100  trials\n",
      "#It:  6600  avg reward:  -51.82199733472472  out of  100  trials\n",
      "#It:  6700  avg reward:  -67.01348871914487  out of  100  trials\n",
      "#It:  6800  avg reward:  -47.42779417342718  out of  100  trials\n",
      "#It:  6900  avg reward:  -54.5593603932394  out of  100  trials\n",
      "#It:  7000  avg reward:  -56.2285591863144  out of  100  trials\n",
      "#It:  7100  avg reward:  -60.90093771861134  out of  100  trials\n",
      "#It:  7200  avg reward:  -33.83232364584081  out of  100  trials\n",
      "#It:  7300  avg reward:  -52.96893054406027  out of  100  trials\n",
      "#It:  7400  avg reward:  -51.26676367465184  out of  100  trials\n",
      "#It:  7500  avg reward:  -54.85400906609229  out of  100  trials\n",
      "#It:  7600  avg reward:  -29.43155259307795  out of  100  trials\n",
      "#It:  7700  avg reward:  -46.16996077273524  out of  100  trials\n",
      "#It:  7800  avg reward:  -31.088952021531714  out of  100  trials\n",
      "#It:  7900  avg reward:  -11.563285986426354  out of  100  trials\n",
      "#It:  8000  avg reward:  -11.22763147006911  out of  100  trials\n",
      "#It:  8100  avg reward:  -5.822823736989243  out of  100  trials\n",
      "#It:  8200  avg reward:  -26.1935237179096  out of  100  trials\n",
      "#It:  8300  avg reward:  -20.62898181470223  out of  100  trials\n",
      "#It:  8400  avg reward:  0.15345955536037736  out of  100  trials\n",
      "#It:  8500  avg reward:  -17.862800865203905  out of  100  trials\n",
      "#It:  8600  avg reward:  30.3600174007844  out of  100  trials\n",
      "#It:  8700  avg reward:  12.304193800810829  out of  100  trials\n",
      "#It:  8800  avg reward:  -1.1847332563081523  out of  100  trials\n",
      "#It:  8900  avg reward:  10.818215699815864  out of  100  trials\n",
      "#It:  9000  avg reward:  -9.513260418326361  out of  100  trials\n",
      "#It:  9100  avg reward:  -24.744464051485707  out of  100  trials\n",
      "#It:  9200  avg reward:  -17.04805831824384  out of  100  trials\n",
      "#It:  9300  avg reward:  4.326979104999172  out of  100  trials\n",
      "#It:  9400  avg reward:  31.734799030568233  out of  100  trials\n",
      "#It:  9500  avg reward:  10.944672253623335  out of  100  trials\n",
      "#It:  9600  avg reward:  18.18354150104347  out of  100  trials\n",
      "#It:  9700  avg reward:  23.81123325062494  out of  100  trials\n",
      "#It:  9800  avg reward:  31.580803381712034  out of  100  trials\n",
      "#It:  9900  avg reward:  42.67442235460012  out of  100  trials\n"
     ]
    }
   ],
   "source": [
    "def sarsa_lander(env, seed=None, render=False, num_iter=50, seg=50):\n",
    "    env.seed()\n",
    "\n",
    "    Q = collections.defaultdict(float)\n",
    "\n",
    "    gamma = 0.95\n",
    "\n",
    "    r_seq = []\n",
    "    it_reward = []\n",
    "\n",
    "    for it in range(num_iter):\n",
    "       \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        lr = 0.6\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "        ds = discrete_states(s)\n",
    "        a = policy_explorer(ds, Q, it)\n",
    "\n",
    "        while True:\n",
    "            # use a policy generator to guide sarsa exploration\n",
    "            # step and get feedback\n",
    "            sa = sa_key(ds, a)\n",
    "            sp, r, done, info = env.step(a)\n",
    "            # update corresponding Q\n",
    "            dsp = discrete_states(sp)\n",
    "            ap = policy_explorer(dsp, Q, it)\n",
    "\n",
    "            next_sa = sa_key(dsp, ap)\n",
    "            if not done:\n",
    "                Q[sa] += lr*(r + gamma * Q[next_sa] - Q[sa])\n",
    "            else:\n",
    "                Q[sa] += lr*(r - Q[sa])\n",
    "            ds = dsp\n",
    "            a = ap\n",
    "            total_reward += r\n",
    "            if render and it % seg == 0:\n",
    "                still_open = env.render()\n",
    "                if still_open == False: break\n",
    "            steps += 1\n",
    "            if done or steps > 1000:\n",
    "                it_reward.append(total_reward)\n",
    "                break\n",
    "        if it % seg == 0:\n",
    "            avg_rwd = np.mean(np.array(it_reward))\n",
    "            print(\"#It: \", it, \" avg reward: \", avg_rwd, \" out of \", len(it_reward), \" trials\")\n",
    "            it_reward = []\n",
    "            r_seq.append(avg_rwd)\n",
    "\n",
    "    return Q, r_seq\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "Q, r_seq = sarsa_lander(env, render=False, num_iter=num_iter, seg=100)\n",
    "\n",
    "y = np.array(r_seq)\n",
    "x = np.linspace(0, num_iter, y.shape[0])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.title('Optimized SARSA')\n",
    "plt.savefig(\"Optimized SARSA\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "np.savetxt(\"Optimized Sarsa.txt\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355c33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#It:  0  avg reward:  -99.36960581276084  out of  1  trials\n",
      "#It:  100  avg reward:  -121.8613190458557  out of  100  trials\n",
      "#It:  200  avg reward:  -105.53823648786073  out of  100  trials\n",
      "#It:  300  avg reward:  -99.69452819856653  out of  100  trials\n",
      "#It:  400  avg reward:  -97.94527293860774  out of  100  trials\n",
      "#It:  500  avg reward:  -95.46232142394841  out of  100  trials\n",
      "#It:  600  avg reward:  -117.08708416159566  out of  100  trials\n",
      "#It:  700  avg reward:  -101.33547019958104  out of  100  trials\n",
      "#It:  800  avg reward:  -97.81620456782031  out of  100  trials\n",
      "#It:  900  avg reward:  -110.0181480177172  out of  100  trials\n",
      "#It:  1000  avg reward:  -87.3408086567052  out of  100  trials\n",
      "#It:  1100  avg reward:  -95.33641048011064  out of  100  trials\n",
      "#It:  1200  avg reward:  -100.48655821225995  out of  100  trials\n",
      "#It:  1300  avg reward:  -122.668858203247  out of  100  trials\n",
      "#It:  1400  avg reward:  -102.5334333182834  out of  100  trials\n",
      "#It:  1500  avg reward:  -110.37799795442085  out of  100  trials\n",
      "#It:  1600  avg reward:  -121.65558227610337  out of  100  trials\n",
      "#It:  1700  avg reward:  -92.12235556071182  out of  100  trials\n",
      "#It:  1800  avg reward:  -90.39692871044845  out of  100  trials\n",
      "#It:  1900  avg reward:  -97.34604894045638  out of  100  trials\n",
      "#It:  2000  avg reward:  -97.01208448620841  out of  100  trials\n",
      "#It:  2100  avg reward:  -119.59946211959931  out of  100  trials\n",
      "#It:  2200  avg reward:  -101.04523072251578  out of  100  trials\n",
      "#It:  2300  avg reward:  -114.0663985582401  out of  100  trials\n",
      "#It:  2400  avg reward:  -107.3492157366044  out of  100  trials\n",
      "#It:  2500  avg reward:  -117.82212567814096  out of  100  trials\n",
      "#It:  2600  avg reward:  -98.71712736229219  out of  100  trials\n",
      "#It:  2700  avg reward:  -118.16165403593428  out of  100  trials\n",
      "#It:  2800  avg reward:  -99.70330557498455  out of  100  trials\n",
      "#It:  2900  avg reward:  -115.46030364475797  out of  100  trials\n",
      "#It:  3000  avg reward:  -104.01091635253562  out of  100  trials\n",
      "#It:  3100  avg reward:  -96.7437827481157  out of  100  trials\n",
      "#It:  3200  avg reward:  -98.63084573236009  out of  100  trials\n",
      "#It:  3300  avg reward:  -78.58772252267777  out of  100  trials\n",
      "#It:  3400  avg reward:  -83.98667943801337  out of  100  trials\n",
      "#It:  3500  avg reward:  -94.92058093315289  out of  100  trials\n",
      "#It:  3600  avg reward:  -94.65532447204586  out of  100  trials\n",
      "#It:  3700  avg reward:  -105.3207437647513  out of  100  trials\n",
      "#It:  3800  avg reward:  -87.46698882915226  out of  100  trials\n",
      "#It:  3900  avg reward:  -71.6390589103492  out of  100  trials\n",
      "#It:  4000  avg reward:  -97.19144939674352  out of  100  trials\n",
      "#It:  4100  avg reward:  -85.98841308586907  out of  100  trials\n",
      "#It:  4200  avg reward:  -95.90382905291277  out of  100  trials\n",
      "#It:  4300  avg reward:  -95.48349546215015  out of  100  trials\n",
      "#It:  4400  avg reward:  -79.86969848548057  out of  100  trials\n",
      "#It:  4500  avg reward:  -78.49576046268857  out of  100  trials\n",
      "#It:  4600  avg reward:  -84.3516792124482  out of  100  trials\n",
      "#It:  4700  avg reward:  -75.0868647388956  out of  100  trials\n",
      "#It:  4800  avg reward:  -85.586593752924  out of  100  trials\n",
      "#It:  4900  avg reward:  -81.30153013490987  out of  100  trials\n",
      "#It:  5000  avg reward:  -56.06381088362768  out of  100  trials\n",
      "#It:  5100  avg reward:  -65.72057848210008  out of  100  trials\n",
      "#It:  5200  avg reward:  -74.3347087101034  out of  100  trials\n",
      "#It:  5300  avg reward:  -63.74258612764794  out of  100  trials\n",
      "#It:  5400  avg reward:  -58.96601359770435  out of  100  trials\n",
      "#It:  5500  avg reward:  -64.97824324766269  out of  100  trials\n",
      "#It:  5600  avg reward:  -76.10695580840752  out of  100  trials\n",
      "#It:  5700  avg reward:  -77.57852727159725  out of  100  trials\n",
      "#It:  5800  avg reward:  -56.95650387624495  out of  100  trials\n",
      "#It:  5900  avg reward:  -75.61328527853851  out of  100  trials\n",
      "#It:  6000  avg reward:  -89.59558112725537  out of  100  trials\n",
      "#It:  6100  avg reward:  -67.18699246596623  out of  100  trials\n",
      "#It:  6200  avg reward:  -62.539050263413465  out of  100  trials\n",
      "#It:  6300  avg reward:  -81.95039380658918  out of  100  trials\n",
      "#It:  6400  avg reward:  -72.67958701616456  out of  100  trials\n",
      "#It:  6500  avg reward:  -47.16007250735357  out of  100  trials\n",
      "#It:  6600  avg reward:  -45.989403249299876  out of  100  trials\n",
      "#It:  6700  avg reward:  -48.47362636135991  out of  100  trials\n",
      "#It:  6800  avg reward:  -50.80586763359371  out of  100  trials\n",
      "#It:  6900  avg reward:  -60.887109758783936  out of  100  trials\n",
      "#It:  7000  avg reward:  -66.65073737802406  out of  100  trials\n",
      "#It:  7100  avg reward:  -46.92061724945006  out of  100  trials\n",
      "#It:  7200  avg reward:  -58.64332304175069  out of  100  trials\n",
      "#It:  7300  avg reward:  -46.62956851862032  out of  100  trials\n",
      "#It:  7400  avg reward:  -46.61530761259979  out of  100  trials\n",
      "#It:  7500  avg reward:  -41.404272301602404  out of  100  trials\n",
      "#It:  7600  avg reward:  -35.54982768036471  out of  100  trials\n",
      "#It:  7700  avg reward:  -43.05223742670978  out of  100  trials\n",
      "#It:  7800  avg reward:  -43.648533637536694  out of  100  trials\n",
      "#It:  7900  avg reward:  -21.05901272406183  out of  100  trials\n",
      "#It:  8000  avg reward:  -39.40042395726164  out of  100  trials\n",
      "#It:  8100  avg reward:  2.5613538087060976  out of  100  trials\n",
      "#It:  8200  avg reward:  -19.96299564504168  out of  100  trials\n",
      "#It:  8300  avg reward:  7.195636167300678  out of  100  trials\n",
      "#It:  8400  avg reward:  -4.024692049347862  out of  100  trials\n",
      "#It:  8500  avg reward:  1.6913953234960812  out of  100  trials\n",
      "#It:  8600  avg reward:  7.328118823242358  out of  100  trials\n",
      "#It:  8700  avg reward:  16.586176157855192  out of  100  trials\n",
      "#It:  8800  avg reward:  32.89063619939353  out of  100  trials\n",
      "#It:  8900  avg reward:  19.695392984232242  out of  100  trials\n",
      "#It:  9000  avg reward:  21.45694967110755  out of  100  trials\n",
      "#It:  9100  avg reward:  20.841976558669675  out of  100  trials\n",
      "#It:  9200  avg reward:  -4.717821320141948  out of  100  trials\n",
      "#It:  9300  avg reward:  26.0699137670286  out of  100  trials\n",
      "#It:  9400  avg reward:  34.473763464773505  out of  100  trials\n",
      "#It:  9500  avg reward:  24.705144253207326  out of  100  trials\n",
      "#It:  9600  avg reward:  39.403269016396706  out of  100  trials\n",
      "#It:  9700  avg reward:  63.277278492191215  out of  100  trials\n",
      "#It:  9800  avg reward:  52.91094169429136  out of  100  trials\n",
      "#It:  9900  avg reward:  69.93492156758481  out of  100  trials\n"
     ]
    }
   ],
   "source": [
    "def qlearning_lander(env, learningrate=0.01, render=True, num_iter=100, seg=100):\n",
    "    Q = collections.defaultdict(float)\n",
    "    gamma = 0.95\n",
    "    r_seq = []\n",
    "    it_reward = []\n",
    "\n",
    "    for it in range(num_iter):\n",
    "        # initialize variables\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "\n",
    "        ds = discrete_states(s)\n",
    "        a = policy_explorer(ds, Q, it)\n",
    "        while True:\n",
    "            a = policy_explorer(ds, Q, it)\n",
    "            sa = sa_key(ds, a)\n",
    "            if render:\n",
    "                env.render()\n",
    "            sp, r, done, info = env.step(a)\n",
    "            # update corresponding Q\n",
    "            dsp = discrete_states(sp)\n",
    "            a_max = policy_explorer(dsp, Q, it, epsilon=0) #pure-greedy\n",
    "            next_sa = sa_key(dsp, a_max)\n",
    "            if not done:\n",
    "                Q[sa] += learningrate*(r + gamma * Q[next_sa] - Q[sa])\n",
    "            else:\n",
    "                Q[sa] += learningrate*(r - Q[sa])\n",
    "            ds = dsp\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            if done or steps > 1000:\n",
    "                it_reward.append(total_reward)\n",
    "                break\n",
    "        if it % seg == 0:\n",
    "            avg_rwd = np.mean(np.array(it_reward))\n",
    "            print(\"#It: \", it, \" avg reward: \", avg_rwd, \" out of \", len(it_reward), \" trials\")\n",
    "            it_reward = []\n",
    "            r_seq.append(avg_rwd)\n",
    "\n",
    "    return Q, r_seq\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "alphas = [0.6]\n",
    "for a in alphas:\n",
    "    Q, r_seq = qlearning_lander(env, learningrate=a, render=False, num_iter=num_iter, seg=100)\n",
    "\n",
    "y = np.array(r_seq)\n",
    "x = np.linspace(0, num_iter, y.shape[0])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.title('Optimized Discretization Q-Learning')\n",
    "plt.savefig(\"Optimized Discretization Q-Learning\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "np.savetxt(\"Optimized Q-Learning.txt\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2e192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
